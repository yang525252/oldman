<!DOCTYPE HTML>
<script src="dist/human.js"></script>

<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Look how ugly are u</title>
  <style>
    #video {
      transform: scaleX(-1); /* 反轉視頻，像鏡子一樣 */
      float: left;
    }
    #output {
      position: absolute;
      bottom: 10px;
      left: 10px;
      color: red;
      font-size: 24px;
    }
    #chat {
      float: right;
      width: 300px;
      padding: 10px;
    }
    #chatLog {
      border: 1px solid black;
      height: 300px;
      overflow-y: auto;
      margin-bottom: 10px;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
    }
  </style>
</head>
<body>
  <h1>mdfk 11</h1>

  <!-- 左側顯示即時情緒偵測 -->
  <video id="video" width="640" height="480" autoplay muted></video>
  <canvas id="canvas" width="640" height="480" style="display:none"></canvas>
  <div id="output">偵測中...</div>

  <!-- 模型載入狀態 -->
  <p id="model-status">正在載入模型...</p>

  <!-- 右側聊天區 -->
  <div id="chat">
    <div id="chatLog"></div>
    <input type="text" id="message" placeholder="輸入訊息">
    <button id="send">送出</button> <!-- 更正按鈕的 id -->
    <button id="voice">語音輸入</button> <!-- 更正按鈕的 id -->
  </div>

  
  
  <script>
    // 情緒的中文翻譯對應表
    const emotionTranslations = {
      neutral: "中立",
      happy: "快樂",
      sad: "悲傷",
      angry: "憤怒",
      fearful: "恐懼",
      disgusted: "厭惡",
      surprised: "驚訝"
    };

    let lastEmotion = "偵測中...";
    let detectedEmotion = "偵測中...";

    // 語音輸入初始化
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'zh-TW';

    // 更新狀態信息的函數
    function updateStatus(status) {
      const statusElement = document.getElementById('model-status');
      statusElement.textContent = status;
    }

    // 設置相機和人臉偵測
    async function setupCamera() {
      try {
        const video = document.getElementById('video');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        video.srcObject = stream;
        return new Promise((resolve) => {
          video.onloadedmetadata = () => {
            resolve(video);
          };
        });
      } catch (error) {
        console.error("Error accessing the camera and microphone:", error);
        alert("無法訪問相機或麥克風，請檢查瀏覽器設定並允許授權。");
      }
    }

    // 偵測情緒
    async function detectEmotion(human, video, canvas) {
      const context = canvas.getContext('2d');
      setInterval(async () => {
        context.drawImage(video, 0, 0, canvas.width, canvas.height);
        const result = await human.detect(canvas);
        const emotions = result.face?.[0]?.emotion;
        const faceBox = result.face?.[0]?.box;

        if (emotions && faceBox) {
          const sortedEmotions = emotions.sort((a, b) => b.score - a.score);
          detectedEmotion = sortedEmotions[0].emotion;
          lastEmotion = `${emotionTranslations[detectedEmotion]}`;
          context.strokeStyle = 'green'; // 綠色框架
          context.lineWidth = 3;
          context.strokeRect(faceBox[0], faceBox[1], faceBox[2], faceBox[3]); // 繪製人臉框
        } else {
          detectedEmotion = '未檢測到人臉';
        }
      }, 1500); // 每 1.5 秒偵測一次
    }

    // 每 5 秒更新顯示
    setInterval(() => {
      document.getElementById('output').textContent = lastEmotion;
      lastEmotion = detectedEmotion;
    }, 5000); // 每 5 秒更新顯示

    // 發送聊天訊息
    async function sendMessage() {
      const message = document.getElementById('message').value;
      const chatLog = document.getElementById('chatLog');
      chatLog.innerHTML += `<p><b>You:</b> ${message}</p>`;
      const emotionMessage = `我現在感覺${lastEmotion}`;

      // 模擬發送到 API，將情緒和訊息一同發送
      const response = await fetch('https://api-endpoint', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ message: message, emotion: lastEmotion })
      });

      const data = await response.json();
      chatLog.innerHTML += `<p><b>Bot:</b> ${data.response}</p>`;
      document.getElementById('message').value = ''; // 清空輸入框
    }

    // 語音輸入功能
    function startVoiceInput() {
      recognition.start();
      recognition.onresult = function(event) {
        const transcript = event.results[0][0].transcript;
        document.getElementById('message').value = transcript;
      };
    }

    // 初始化 Human 庫並監控模型載入進度
    async function loadModels() {
      const human = new Human.Human({
        modelBasePath: 'https://cdn.jsdelivr.net/npm/@vladmandic/human/models',
        face: { enabled: true, emotion: { enabled: true } },
      });

      // 監控模型載入進度
      human.events.on('progress', (event) => {
        updateStatus(`正在載入: ${event.detail.model} (${event.detail.loaded}/${event.detail.total})`);
      });

      // 載入模型
      await human.load();

      // 模型載入完成後
      updateStatus('所有模型已加載完成！');
      return human;
    }

    // 主函數
    async function main() {
      const video = await setupCamera(); // 啟動相機
      const canvas = document.getElementById('canvas');

      // 等待模型載入完成後再進行情緒偵測
      const human = await loadModels();

      // 啟動情緒偵測
      detectEmotion(human, video, canvas);
    }

    main(); // 啟動主流程

    // 按鈕綁定事件
    document.getElementById('send').addEventListener('click', sendMessage);
    document.getElementById('voice').addEventListener('click', startVoiceInput);
  </script>
</body>
</html>
