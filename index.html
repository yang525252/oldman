<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>test1</title>
  <!-- 引入 Human 庫 -->
  <script src="https://cdn.jsdelivr.net/npm/@vladmandic/human/dist/human.js"></script>
  <style>
    /* 隱藏視頻元素，但保留其功能 */
    #video {
      position: absolute;
      width: 1px;
      height: 1px;
    }
  </style>
</head>
<body>
  
  <!-- 視頻元素，用於顯示攝像頭畫面 -->
  <video id="video" width="320" height="240" autoplay muted playsinline></video>
  <!-- 畫布，用於繪製檢測結果 -->
  <canvas id="canvas" width="320" height="240"></canvas>
  <!-- 假設 iframe 的 id 是 'gradio_iframe' -->
  <iframe id="gradio_iframe" src="https://yang5252-oldman2.hf.space" style="width:100%; height:600px;"></iframe>

  <script>
    // 在情緒檢測程式碼中，當檢測到情緒時：
    function sendEmotionToGradio(emotion) {
      const iframe = document.getElementById('gradio_iframe');
      iframe.contentWindow.postMessage({ emotion: emotion }, 'https://yang5252-oldman2.hf.space');
    }
  </script>

  <script>
    // 配置 Human 庫
    const humanConfig = {
      modelBasePath: 'https://cdn.jsdelivr.net/npm/@vladmandic/human/models',
      face: {
        enabled: true,
        detector: { enabled: true },
        mesh: { enabled: false },
        iris: { enabled: false },
        emotion: { enabled: true },
        description: { enabled: false },
        age: { enabled: false },
        gender: { enabled: false },
      },
    };

    const human = new Human.Human(humanConfig);

    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    let lastEmotion = '';
    let emotionTimestamp = 0;

    // 設置攝像頭
    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    // 渲染函數，進行實時檢測和繪製
    async function render() {
      const result = await human.detect(video);

      // 繪製攝像頭畫面
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      // 繪製檢測結果
      if (result.face && result.face.length > 0) {
        for (let face of result.face) {
          const { box, emotion } = face;
          // 繪製人臉邊框
          ctx.strokeStyle = 'red';
          ctx.lineWidth = 2;
          ctx.strokeRect(box[0], box[1], box[2], box[3]);
          // 顯示情緒文字
          if (emotion && emotion.length > 0) {
            const emotions = emotion.map((e) => `${e.emotion}: ${e.score.toFixed(2)}`).join(', ');
            lastEmotion = emotions;
            emotionTimestamp = Date.now();
            ctx.fillStyle = 'red';
            ctx.font = '16px Arial';
            ctx.fillText(emotions, box[0], box[1] - 10);

            // 發送情緒數據到 Gradio 應用
            sendEmotionToGradio(emotions);
          }
        }
      }
      
      // 在畫面上顯示情緒文字，持續至少5秒
      if (lastEmotion && Date.now() - emotionTimestamp < 5000) {
        ctx.fillStyle = 'red';
        ctx.font = '16px Arial';
        ctx.fillText(lastEmotion, 10, 20);
      }

      requestAnimationFrame(render);
    }

    // 初始化函數
    (async () => {
      await human.load();
      await human.warmup(); // 可選，預加載模型以提高性能
      await setupCamera();
      render();
    })();
  </script>
</body>
</html>
