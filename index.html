<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>look how ugly are u</title>
  <style>
    #video {
      transform: scaleX(-1); /* 反轉視頻，像鏡子一樣 */
      float: left;
    }
    #output {
      position: absolute;
      bottom: 10px;
      left: 10px;
      color: red;
      font-size: 24px;
    }
    #chat {
      float: right;
      width: 300px;
      padding: 10px;
    }
    #chatLog {
      border: 1px solid black;
      height: 300px;
      overflow-y: auto;
      margin-bottom: 10px;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
    }
  </style>
</head>
<body>
  <h1>Real-time Emotion Detection and Chat</h1>
  
  <!-- 左側顯示即時情緒偵測 -->
  <video id="video" width="640" height="480" autoplay muted></video>
  <canvas id="canvas" width="640" height="480" style="display:none"></canvas>
  <div id="output">偵測中...</div>

  <!-- 右側聊天區 -->
  <div id="chat">
    <div id="chatLog"></div>
    <input type="text" id="message" placeholder="輸入訊息">
    <button onclick="sendMessage()">送出</button>
    <button onclick="startVoiceInput()">語音輸入</button>
  </div>

  <!-- 引入 Human Library 和語音 API -->
  <script src="https://cdn.jsdelivr.net/npm/@vladmandic/human"></script>

  <script>
    // 情緒的中文翻譯對應表
    const emotionTranslations = {
      neutral: "中立",
      happy: "快樂",
      sad: "悲傷",
      angry: "憤怒",
      fearful: "恐懼",
      disgusted: "厭惡",
      surprised: "驚訝"
    };
    let lastEmotion = "偵測中...";

    // 語音輸入初始化
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'zh-TW';

    // 設置相機和人臉偵測
    async function setupCamera() {
      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    // 偵測情緒
    async function detectEmotion(human, video, canvas) {
      const context = canvas.getContext('2d');
      setInterval(async () => {
        context.drawImage(video, 0, 0, canvas.width, canvas.height);
        const result = await human.detect(canvas);
        const emotions = result.face?.[0]?.emotion;
        if (emotions) {
          const sortedEmotions = emotions.sort((a, b) => b.score - a.score);
          const detectedEmotion = sortedEmotions[0].emotion;
          lastEmotion = `${emotionTranslations[detectedEmotion]}`;
          context.strokeStyle = 'green';//綠臉
          context.lineWidth = 3;
          context.strokeRect(faceBox[0], faceBox[1], faceBox[2], faceBox[3]);
        } else {
          lastEmotion = '未檢測到人臉';
        }
        document.getElementById('output').textContent = lastEmotion;
      }, 1500);
        } else {
          lastEmotion = '未檢測到人臉';
        }
        document.getElementById('output').textContent = lastEmotion;
      }, 1500);
    }
    const apiKey = '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";
    // 發送聊天訊息
    async function sendMessage() {
      const message = document.getElementById('message').value;
      const chatLog = document.getElementById('chatLog');
      
      // 顯示用戶消息並附帶偵測情緒
      chatLog.innerHTML += `<p><b>You:</b> ${message}</p>`;
      
      // 模擬發送到伺服器，伺服器可以接收「我現在感覺xxx」
      const emotionMessage = `我現在感覺${lastEmotion}`;
      
      // 模擬伺服器回應
      const botResponse = `Bot 回應 (情緒為: ${lastEmotion})`;
      chatLog.innerHTML += `<p><b>Bot:</b> ${botResponse}</p>`;
      
      // 清空輸入框
      document.getElementById('message').value = '';
    }

    // 語音輸入功能
    function startVoiceInput() {
      recognition.start();
      recognition.onresult = function(event) {
        const transcript = event.results[0][0].transcript;
        document.getElementById('message').value = transcript;
      };
    }

    // 初始化 Human 和執行主要函數
    async function main() {
      const video = await setupCamera();
      const canvas = document.getElementById('canvas');
      const human = new Human.Human({
        modelBasePath: 'https://cdn.jsdelivr.net/npm/@vladmandic/human/models',
        face: { enabled: true, emotion: { enabled: true } },
      });
      await human.load();
      detectEmotion(human, video, canvas);
    }

    main();
  </script>
</body>
</html>
